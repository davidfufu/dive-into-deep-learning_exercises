{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eca70952",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b01bacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, folder, sha1_hash=None):\n",
    "    \"\"\"Download a file to folder and return the local filepath.\"\"\"\n",
    "\n",
    "def extract(filename, folder):\n",
    "    \"\"\"Extract a zip/tar file into folder.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4c6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleHouse(d2l.DataModule):\n",
    "    def __init__(self, batch_size, train=None, val=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if self.train is None:\n",
    "            self.raw_train = pd.read_csv(d2l.download(\n",
    "                d2l.DATA_URL + 'kaggle_house_pred_train.csv', self.root,\n",
    "                sha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))\n",
    "            self.raw_val = pd.read_csv(d2l.download(\n",
    "                d2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,\n",
    "                sha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9a70569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n"
     ]
    }
   ],
   "source": [
    "data = KaggleHouse(batch_size=64)\n",
    "print(data.raw_train.shape)\n",
    "print(data.raw_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6409ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n",
      "0   1          60       RL         65.0       WD        Normal     208500\n",
      "1   2          20       RL         80.0       WD        Normal     181500\n",
      "2   3          60       RL         68.0       WD        Normal     223500\n",
      "3   4          70       RL         60.0       WD       Abnorml     140000\n"
     ]
    }
   ],
   "source": [
    "print(data.raw_train.iloc[:4, [0, 1, 2, 3, -3, -2, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef92a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(KaggleHouse)\n",
    "def preprocess(self):\n",
    "    # Remove the ID and label columns\n",
    "    label = 'SalePrice'\n",
    "    features = pd.concat(\n",
    "        (self.raw_train.drop(columns=['Id', label]),\n",
    "         self.raw_val.drop(columns=['Id'])))\n",
    "    # Standardize numerical columns\n",
    "    numeric_features = features.dtypes[features.dtypes!='object'].index\n",
    "    features[numeric_features] = features[numeric_features].apply(\n",
    "        lambda x: (x - x.mean()) / (x.std()))\n",
    "    # Replace NAN numerical features by 0\n",
    "    features[numeric_features] = features[numeric_features].fillna(0)\n",
    "    # Replace discrete features by one-hot encoding\n",
    "    features = pd.get_dummies(features, dummy_na=True)\n",
    "    # Save preprocessed features\n",
    "    self.train = features[:self.raw_train.shape[0]].copy()\n",
    "    self.train[label] = self.raw_train[label]\n",
    "    self.val = features[self.raw_train.shape[0]:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de78f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 331)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.preprocess()\n",
    "data.train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55d9ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(KaggleHouse)\n",
    "def get_dataloader(self, train):\n",
    "    label = 'SalePrice'\n",
    "    data = self.train if train else self.val\n",
    "    if label not in data: return\n",
    "    get_tensor = lambda x: torch.tensor(x.values, dtype=torch.float32)\n",
    "    # Logarithm of prices\n",
    "    tensors = (get_tensor(data.drop(columns=[label])),  # X\n",
    "               torch.log(get_tensor(data[label])).reshape((-1, 1)))  # Y\n",
    "    return self.get_tensorloader(tensors, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a981bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_data(data, k):\n",
    "    rets = []\n",
    "    fold_size = data.train.shape[0] // k\n",
    "    for j in range(k):\n",
    "        idx = range(j * fold_size, (j+1) * fold_size)\n",
    "        rets.append(KaggleHouse(data.batch_size, data.train.drop(index=idx),\n",
    "                                data.train.loc[idx]))\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bffd4ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(trainer, data, k, lr):\n",
    "    val_loss, models = [], []\n",
    "    for i, data_fold in enumerate(k_fold_data(data, k)):\n",
    "        model = d2l.LinearRegression(lr)\n",
    "        model.board.yscale='log'\n",
    "        if i != 0: model.board.display = False\n",
    "        trainer.fit(model, data_fold)\n",
    "        val_loss.append(float(model.board.data['val_loss'][-1].y))\n",
    "        models.append(model)\n",
    "    print(f'average validation log mse = {sum(val_loss)/len(val_loss)}')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b142d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m, in \u001b[0;36mk_fold\u001b[1;34m(trainer, data, k, lr)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39myscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: model\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mdisplay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m val_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(model\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39my))\n\u001b[0;32m      9\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\d2l\\torch.py:267\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, data)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, data):\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_model(model)\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfigure_optimizers()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\d2l\\torch.py:255\u001b[0m, in \u001b[0;36mTrainer.prepare_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mval_dataloader()\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\d2l\\torch.py:236\u001b[0m, in \u001b[0;36mDataModule.train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m      6\u001b[0m get_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mtensor(x\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Logarithm of prices\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tensors \u001b[38;5;241m=\u001b[39m (\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# X\u001b[39;00m\n\u001b[0;32m      9\u001b[0m            torch\u001b[38;5;241m.\u001b[39mlog(get_tensor(data[label]))\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))  \u001b[38;5;66;03m# Y\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tensorloader(tensors, train)\n",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mget_dataloader.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m get_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Logarithm of prices\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tensors \u001b[38;5;241m=\u001b[39m (get_tensor(data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[label])),  \u001b[38;5;66;03m# X\u001b[39;00m\n\u001b[0;32m      9\u001b[0m            torch\u001b[38;5;241m.\u001b[39mlog(get_tensor(data[label]))\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))  \u001b[38;5;66;03m# Y\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "models = k_fold(trainer, data, k=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "814bb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(KaggleHouse)\n",
    "def get_dataloader(self, train):\n",
    "    label = 'SalePrice'\n",
    "    data = self.train if train else self.val\n",
    "    if label not in data: return\n",
    "    get_tensor = lambda x: torch.tensor(x.values, dtype=torch.float32)\n",
    "    # Logarithm of prices\n",
    "    tensors = (get_tensor(data.drop(columns=[label])),  # X\n",
    "               torch.log(get_tensor(data[label])).reshape((-1, 1)))  # Y\n",
    "    return self.get_tensorloader(tensors, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "741504e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_data(data, k):\n",
    "    rets = []\n",
    "    fold_size = data.train.shape[0] // k\n",
    "    for j in range(k):\n",
    "        idx = range(j * fold_size, (j+1) * fold_size)\n",
    "        rets.append(KaggleHouse(data.batch_size, data.train.drop(index=idx),\n",
    "                                data.train.loc[idx]))\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a87e8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(trainer, data, k, lr):\n",
    "    val_loss, models = [], []\n",
    "    for i, data_fold in enumerate(k_fold_data(data, k)):\n",
    "        model = d2l.LinearRegression(lr)\n",
    "        model.board.yscale='log'\n",
    "        if i != 0: model.board.display = False\n",
    "        trainer.fit(model, data_fold)\n",
    "        val_loss.append(float(model.board.data['val_loss'][-1].y))\n",
    "        models.append(model)\n",
    "    print(f'average validation log mse = {sum(val_loss)/len(val_loss)}')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00cc3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m, in \u001b[0;36mk_fold\u001b[1;34m(trainer, data, k, lr)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39myscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: model\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mdisplay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m val_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(model\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39my))\n\u001b[0;32m      9\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\d2l\\torch.py:267\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, data)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, data):\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_model(model)\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfigure_optimizers()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\d2l\\torch.py:255\u001b[0m, in \u001b[0;36mTrainer.prepare_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mval_dataloader()\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\d2l\\torch.py:236\u001b[0m, in \u001b[0;36mDataModule.train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m      6\u001b[0m get_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mtensor(x\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Logarithm of prices\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tensors \u001b[38;5;241m=\u001b[39m (\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# X\u001b[39;00m\n\u001b[0;32m      9\u001b[0m            torch\u001b[38;5;241m.\u001b[39mlog(get_tensor(data[label]))\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))  \u001b[38;5;66;03m# Y\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tensorloader(tensors, train)\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mget_dataloader.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m get_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Logarithm of prices\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tensors \u001b[38;5;241m=\u001b[39m (get_tensor(data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[label])),  \u001b[38;5;66;03m# X\u001b[39;00m\n\u001b[0;32m      9\u001b[0m            torch\u001b[38;5;241m.\u001b[39mlog(get_tensor(data[label]))\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))  \u001b[38;5;66;03m# Y\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "models = k_fold(trainer, data, k=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27343e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
