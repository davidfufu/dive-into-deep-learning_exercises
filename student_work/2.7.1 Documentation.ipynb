{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f11c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3bea9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AVG', 'AggregationType', 'AliasDb', 'AnyType', 'Argument', 'ArgumentSpec', 'BFloat16Storage', 'BFloat16Tensor', 'BenchmarkConfig', 'BenchmarkExecutionStats', 'Block', 'BoolStorage', 'BoolTensor', 'BoolType', 'BufferDict', 'ByteStorage', 'ByteTensor', 'CONV_BN_FUSION', 'CallStack', 'Callable', 'Capsule', 'CharStorage', 'CharTensor', 'ClassType', 'Code', 'CompilationUnit', 'CompleteArgumentSpec', 'ComplexDoubleStorage', 'ComplexFloatStorage', 'ComplexType', 'ConcreteModuleType', 'ConcreteModuleTypeBuilder', 'DeepCopyMemoTable', 'DeserializationStorageContext', 'DeviceObjType', 'DictType', 'DisableTorchFunction', 'DoubleStorage', 'DoubleTensor', 'EnumType', 'ErrorReport', 'ExecutionPlan', 'FUSE_ADD_RELU', 'FatalError', 'FileCheck', 'FloatStorage', 'FloatTensor', 'FloatType', 'FunctionSchema', 'Future', 'FutureType', 'Generator', 'Gradient', 'Graph', 'GraphExecutorState', 'HOIST_CONV_PACKED_PARAMS', 'HalfStorage', 'HalfTensor', 'INSERT_FOLD_PREPACK_OPS', 'IODescriptor', 'InferredType', 'IntStorage', 'IntTensor', 'IntType', 'InterfaceType', 'JITException', 'ListType', 'LiteScriptModule', 'LockingLogger', 'LoggerBase', 'LongStorage', 'LongTensor', 'MobileOptimizerType', 'ModuleDict', 'Node', 'NoneType', 'NoopLogger', 'NumberType', 'OperatorInfo', 'OptionalType', 'PRIVATE_OPS', 'ParameterDict', 'PyObjectType', 'PyTorchFileReader', 'PyTorchFileWriter', 'QInt32Storage', 'QInt8Storage', 'QUInt2x4Storage', 'QUInt4x2Storage', 'QUInt8Storage', 'REMOVE_DROPOUT', 'RRefType', 'SUM', 'ScriptClass', 'ScriptClassFunction', 'ScriptDict', 'ScriptDictIterator', 'ScriptDictKeyIterator', 'ScriptFunction', 'ScriptList', 'ScriptListIterator', 'ScriptMethod', 'ScriptModule', 'ScriptModuleSerializer', 'ScriptObject', 'ScriptObjectProperty', 'SerializationStorageContext', 'Set', 'ShortStorage', 'ShortTensor', 'Size', 'StaticModule', 'Storage', 'StorageBase', 'Stream', 'StreamObjType', 'StringType', 'SymIntType', 'TYPE_CHECKING', 'Tensor', 'TensorType', 'ThroughputBenchmark', 'TracingState', 'TupleType', 'Type', 'USE_GLOBAL_DEPS', 'USE_RTLD_GLOBAL_WITH_LIBTORCH', 'Union', 'UnionType', 'Use', 'Value', '_C', '_TypedStorage', '_UntypedStorage', '_VF', '__all__', '__annotations__', '__builtins__', '__cached__', '__config__', '__doc__', '__file__', '__future__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_adaptive_avg_pool2d', '_adaptive_avg_pool3d', '_add_batch_dim', '_add_relu', '_add_relu_', '_addmm_activation', '_aminmax', '_amp_foreach_non_finite_check_and_unscale_', '_amp_update_scale_', '_assert', '_assert_async', '_batch_norm_impl_index', '_cast_Byte', '_cast_Char', '_cast_Double', '_cast_Float', '_cast_Half', '_cast_Int', '_cast_Long', '_cast_Short', '_choose_qparams_per_tensor', '_classes', '_coalesce', '_compute_linear_combination', '_conj', '_conj_copy', '_conj_physical', '_convert_indices_from_coo_to_csr', '_convert_indices_from_csr_to_coo', '_convolution', '_convolution_mode', '_copy_from', '_copy_from_and_resize', '_ctc_loss', '_cudnn_ctc_loss', '_cudnn_init_dropout_state', '_cudnn_rnn', '_cudnn_rnn_flatten_weight', '_cufft_clear_plan_cache', '_cufft_get_plan_cache_max_size', '_cufft_get_plan_cache_size', '_cufft_set_plan_cache_max_size', '_cummax_helper', '_cummin_helper', '_debug_has_internal_overlap', '_det_lu_based_helper', '_det_lu_based_helper_backward_helper', '_dim_arange', '_dirichlet_grad', '_disable_functionalization', '_efficientzerotensor', '_embedding_bag', '_embedding_bag_forward_only', '_empty_affine_quantized', '_empty_per_channel_affine_quantized', '_enable_functionalization', '_euclidean_dist', '_fake_quantize_learnable_per_channel_affine', '_fake_quantize_learnable_per_tensor_affine', '_fake_quantize_per_tensor_affine_cachemask_tensor_qparams', '_fft_c2c', '_fft_c2r', '_fft_r2c', '_foreach_abs', '_foreach_abs_', '_foreach_acos', '_foreach_acos_', '_foreach_add', '_foreach_add_', '_foreach_addcdiv', '_foreach_addcdiv_', '_foreach_addcmul', '_foreach_addcmul_', '_foreach_asin', '_foreach_asin_', '_foreach_atan', '_foreach_atan_', '_foreach_ceil', '_foreach_ceil_', '_foreach_cos', '_foreach_cos_', '_foreach_cosh', '_foreach_cosh_', '_foreach_div', '_foreach_div_', '_foreach_erf', '_foreach_erf_', '_foreach_erfc', '_foreach_erfc_', '_foreach_exp', '_foreach_exp_', '_foreach_expm1', '_foreach_expm1_', '_foreach_floor', '_foreach_floor_', '_foreach_frac', '_foreach_frac_', '_foreach_lgamma', '_foreach_lgamma_', '_foreach_log', '_foreach_log10', '_foreach_log10_', '_foreach_log1p', '_foreach_log1p_', '_foreach_log2', '_foreach_log2_', '_foreach_log_', '_foreach_maximum', '_foreach_minimum', '_foreach_mul', '_foreach_mul_', '_foreach_neg', '_foreach_neg_', '_foreach_norm', '_foreach_reciprocal', '_foreach_reciprocal_', '_foreach_round', '_foreach_round_', '_foreach_sigmoid', '_foreach_sigmoid_', '_foreach_sin', '_foreach_sin_', '_foreach_sinh', '_foreach_sinh_', '_foreach_sqrt', '_foreach_sqrt_', '_foreach_sub', '_foreach_sub_', '_foreach_tan', '_foreach_tan_', '_foreach_tanh', '_foreach_tanh_', '_foreach_trunc', '_foreach_trunc_', '_foreach_zero_', '_from_functional_tensor', '_fused_dropout', '_fused_moving_avg_obs_fq_helper', '_fw_primal_copy', '_grid_sampler_2d_cpu_fallback', '_has_compatible_shallow_copy_type', '_histogramdd_bin_edges', '_histogramdd_from_bin_cts', '_histogramdd_from_bin_tensors', '_import_dotted_name', '_index_put_impl_', '_indices_copy', '_initExtension', '_is_functional_tensor', '_is_zerotensor', '_jit_internal', '_linalg_check_errors', '_linalg_inv_out_helper_', '_linalg_qr_helper', '_linalg_svd', '_linalg_utils', '_load_global_deps', '_lobpcg', '_log_softmax', '_log_softmax_backward_data', '_logcumsumexp', '_lowrank', '_lstm_mps', '_lu_with_info', '_make_dual', '_make_dual_copy', '_make_per_channel_quantized_tensor', '_make_per_tensor_quantized_tensor', '_masked', '_masked_scale', '_masked_softmax', '_meta_registrations', '_mkldnn', '_mkldnn_reshape', '_mkldnn_transpose', '_mkldnn_transpose_', '_mps_convolution', '_mps_convolution_transpose', '_mps_linear_backward_weights', '_mps_max_pool2d', '_namedtensor_internals', '_native_multi_head_attention', '_neg_view', '_neg_view_copy', '_nested_from_padded', '_nested_from_padded_and_nested_example', '_nested_tensor_from_mask', '_nnpack_available', '_nnpack_spatial_convolution', '_ops', '_pack_padded_sequence', '_pad_packed_sequence', '_pin_memory', '_prims', '_register_device_module', '_remove_batch_dim', '_reshape_alias_copy', '_reshape_from_tensor', '_resize_output_', '_rowwise_prune', '_sample_dirichlet', '_saturate_weight_to_fp16', '_shape_as_tensor', '_six', '_sobol_engine_draw', '_sobol_engine_ff_', '_sobol_engine_initialize_state_', '_sobol_engine_scramble_', '_softmax', '_softmax_backward_data', '_sources', '_sparse_broadcast_to', '_sparse_broadcast_to_copy', '_sparse_bsc_tensor_unsafe', '_sparse_bsr_tensor_unsafe', '_sparse_compressed_tensor_unsafe', '_sparse_coo_tensor_unsafe', '_sparse_csc_tensor_unsafe', '_sparse_csr_prod', '_sparse_csr_sum', '_sparse_csr_tensor_unsafe', '_sparse_log_softmax_backward_data', '_sparse_mask_helper', '_sparse_softmax_backward_data', '_sparse_sparse_matmul', '_sparse_sum', '_stack', '_standard_gamma', '_standard_gamma_grad', '_storage_classes', '_string_classes', '_sync', '_tensor', '_tensor_classes', '_tensor_str', '_test_serialization_subcmul', '_to_cpu', '_to_functional_tensor', '_torch_cuda_cu_linker_symbol_op', '_transform_bias_rescale_qkv', '_transformer_encoder_layer_fwd', '_trilinear', '_unique', '_unique2', '_unpack_dual', '_use_cudnn_ctc_loss', '_use_cudnn_rnn_flatten_weight', '_utils', '_utils_internal', '_validate_sparse_bsc_tensor_args', '_validate_sparse_bsr_tensor_args', '_validate_sparse_compressed_tensor_args', '_validate_sparse_coo_tensor_args', '_validate_sparse_csc_tensor_args', '_validate_sparse_csr_tensor_args', '_values_copy', '_vmap_internals', '_weight_norm', '_weight_norm_interface', 'abs', 'abs_', 'absolute', 'acos', 'acos_', 'acosh', 'acosh_', 'adaptive_avg_pool1d', 'adaptive_max_pool1d', 'add', 'addbmm', 'addcdiv', 'addcmul', 'addmm', 'addmv', 'addmv_', 'addr', 'adjoint', 'affine_grid_generator', 'alias_copy', 'align_tensors', 'all', 'allclose', 'alpha_dropout', 'alpha_dropout_', 'amax', 'amin', 'aminmax', 'amp', 'angle', 'any', 'ao', 'arange', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan2', 'arctan_', 'arctanh', 'arctanh_', 'are_deterministic_algorithms_enabled', 'argmax', 'argmin', 'argsort', 'argwhere', 'as_strided', 'as_strided_', 'as_strided_copy', 'as_tensor', 'asarray', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan_', 'atanh', 'atanh_', 'atleast_1d', 'atleast_2d', 'atleast_3d', 'attr', 'autocast', 'autocast_decrement_nesting', 'autocast_increment_nesting', 'autograd', 'avg_pool1d', 'backends', 'baddbmm', 'bartlett_window', 'base_py_dll_path', 'batch_norm', 'batch_norm_backward_elemt', 'batch_norm_backward_reduce', 'batch_norm_elemt', 'batch_norm_gather_stats', 'batch_norm_gather_stats_with_counts', 'batch_norm_stats', 'batch_norm_update_stats', 'bernoulli', 'bfloat16', 'bilinear', 'binary_cross_entropy_with_logits', 'bincount', 'binomial', 'bitwise_and', 'bitwise_left_shift', 'bitwise_not', 'bitwise_or', 'bitwise_right_shift', 'bitwise_xor', 'blackman_window', 'block_diag', 'bmm', 'bool', 'broadcast_shapes', 'broadcast_tensors', 'broadcast_to', 'bucketize', 'builtins', 'can_cast', 'candidate', 'cartesian_prod', 'cat', 'ccol_indices_copy', 'cdist', 'cdouble', 'ceil', 'ceil_', 'celu', 'celu_', 'cfloat', 'chain_matmul', 'chalf', 'channel_shuffle', 'channels_last', 'channels_last_3d', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'choose_qparams_optimized', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'classes', 'classproperty', 'clear_autocast_cache', 'clip', 'clip_', 'clone', 'col_indices_copy', 'column_stack', 'combinations', 'compiled_with_cxx11_abi', 'complex', 'complex128', 'complex32', 'complex64', 'concat', 'conj', 'conj_physical', 'conj_physical_', 'constant_pad_nd', 'contiguous_format', 'conv1d', 'conv2d', 'conv3d', 'conv_tbc', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'convolution', 'copysign', 'corrcoef', 'cos', 'cos_', 'cosh', 'cosh_', 'cosine_embedding_loss', 'cosine_similarity', 'count_nonzero', 'cov', 'cpp', 'cpu', 'cross', 'crow_indices_copy', 'ctc_loss', 'ctypes', 'cuda', 'cuda_path', 'cuda_version', 'cudnn_affine_grid_generator', 'cudnn_batch_norm', 'cudnn_convolution', 'cudnn_convolution_add_relu', 'cudnn_convolution_relu', 'cudnn_convolution_transpose', 'cudnn_grid_sampler', 'cudnn_is_acceptable', 'cummax', 'cummin', 'cumprod', 'cumsum', 'cumulative_trapezoid', 'default_generator', 'deg2rad', 'deg2rad_', 'dequantize', 'det', 'detach', 'detach_', 'detach_copy', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'diagonal_copy', 'diagonal_scatter', 'diff', 'digamma', 'dist', 'distributed', 'distributions', 'div', 'divide', 'dll', 'dll_path', 'dll_paths', 'dlls', 'dot', 'double', 'dropout', 'dropout_', 'dsmm', 'dsplit', 'dstack', 'dtype', 'e', 'eig', 'einsum', 'embedding', 'embedding_bag', 'embedding_renorm_', 'empty', 'empty_like', 'empty_quantized', 'empty_strided', 'enable_grad', 'eq', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'exp', 'exp2', 'exp2_', 'exp_', 'expand_copy', 'expm1', 'expm1_', 'eye', 'fake_quantize_per_channel_affine', 'fake_quantize_per_tensor_affine', 'fbgemm_linear_fp16_weight', 'fbgemm_linear_fp16_weight_fp32_activation', 'fbgemm_linear_int8_weight', 'fbgemm_linear_int8_weight_fp32_activation', 'fbgemm_linear_quantize_weight', 'fbgemm_pack_gemm_matrix_fp16', 'fbgemm_pack_quantized_matrix', 'feature_alpha_dropout', 'feature_alpha_dropout_', 'feature_dropout', 'feature_dropout_', 'fft', 'fill', 'fill_', 'finfo', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'float16', 'float32', 'float64', 'float_power', 'floor', 'floor_', 'floor_divide', 'fmax', 'fmin', 'fmod', 'fork', 'frac', 'frac_', 'frexp', 'frobenius_norm', 'from_dlpack', 'from_file', 'from_numpy', 'frombuffer', 'full', 'full_like', 'functional', 'fused_moving_avg_obs_fake_quant', 'futures', 'gather', 'gcd', 'gcd_', 'ge', 'geqrf', 'ger', 'get_autocast_cpu_dtype', 'get_autocast_gpu_dtype', 'get_default_dtype', 'get_deterministic_debug_mode', 'get_device', 'get_file_path', 'get_float32_matmul_precision', 'get_num_interop_threads', 'get_num_threads', 'get_rng_state', 'glob', 'gradient', 'greater', 'greater_equal', 'grid_sampler', 'grid_sampler_2d', 'grid_sampler_3d', 'group_norm', 'gru', 'gru_cell', 'gt', 'half', 'hamming_window', 'hann_window', 'hardshrink', 'has_cuda', 'has_cudnn', 'has_lapack', 'has_mkl', 'has_mkldnn', 'has_mps', 'has_openmp', 'has_spectral', 'heaviside', 'hinge_embedding_loss', 'histc', 'histogram', 'histogramdd', 'hsmm', 'hsplit', 'hspmm', 'hstack', 'hub', 'hypot', 'i0', 'i0_', 'igamma', 'igammac', 'iinfo', 'imag', 'import_ir_module', 'import_ir_module_from_buffer', 'index_add', 'index_copy', 'index_fill', 'index_put', 'index_put_', 'index_reduce', 'index_select', 'indices_copy', 'inf', 'inference_mode', 'init_num_threads', 'initial_seed', 'inner', 'inspect', 'instance_norm', 'int', 'int16', 'int32', 'int64', 'int8', 'int_repr', 'inverse', 'is_anomaly_enabled', 'is_autocast_cache_enabled', 'is_autocast_cpu_enabled', 'is_autocast_enabled', 'is_complex', 'is_conj', 'is_deterministic_algorithms_warn_only_enabled', 'is_distributed', 'is_floating_point', 'is_grad_enabled', 'is_inference', 'is_inference_mode_enabled', 'is_loaded', 'is_neg', 'is_nonzero', 'is_same_size', 'is_signed', 'is_storage', 'is_tensor', 'is_vulkan_available', 'is_warn_always_enabled', 'isclose', 'isfinite', 'isin', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'jit', 'kaiser_window', 'kernel32', 'kl_div', 'kron', 'kthvalue', 'last_error', 'layer_norm', 'layout', 'lcm', 'lcm_', 'ldexp', 'ldexp_', 'le', 'legacy_contiguous_format', 'lerp', 'less', 'less_equal', 'lgamma', 'library', 'linalg', 'linspace', 'load', 'lobpcg', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'logit', 'logit_', 'logspace', 'logsumexp', 'long', 'lstm', 'lstm_cell', 'lstsq', 'lt', 'lu', 'lu_solve', 'lu_unpack', 'manual_seed', 'margin_ranking_loss', 'masked_fill', 'masked_scatter', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'matrix_rank', 'max', 'max_pool1d', 'max_pool1d_with_indices', 'max_pool2d', 'max_pool3d', 'maximum', 'mean', 'median', 'memory_format', 'merge_type_from_type_comment', 'meshgrid', 'min', 'minimum', 'miopen_batch_norm', 'miopen_convolution', 'miopen_convolution_transpose', 'miopen_depthwise_convolution', 'miopen_rnn', 'mkldnn_adaptive_avg_pool2d', 'mkldnn_convolution', 'mkldnn_linear_backward_weights', 'mkldnn_max_pool2d', 'mkldnn_max_pool3d', 'mm', 'mode', 'moveaxis', 'movedim', 'msort', 'mul', 'multinomial', 'multiply', 'multiprocessing', 'mv', 'mvlgamma', 'name', 'nan', 'nan_to_num', 'nan_to_num_', 'nanmean', 'nanmedian', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'native_batch_norm', 'native_channel_shuffle', 'native_dropout', 'native_group_norm', 'native_layer_norm', 'native_norm', 'ne', 'neg', 'neg_', 'negative', 'negative_', 'nested_tensor', 'nextafter', 'nn', 'no_grad', 'nonzero', 'norm', 'norm_except_dim', 'normal', 'not_equal', 'nuclear_norm', 'numel', 'nvtoolsext_dll_path', 'obj', 'ones', 'ones_like', 'onnx', 'ops', 'optim', 'orgqr', 'ormqr', 'os', 'outer', 'overrides', 'package', 'pairwise_distance', 'parse_ir', 'parse_schema', 'parse_type_comment', 'path_patched', 'pca_lowrank', 'pdist', 'per_channel_affine', 'per_channel_affine_float_qparams', 'per_channel_symmetric', 'per_tensor_affine', 'per_tensor_symmetric', 'permute', 'permute_copy', 'pfiles_path', 'pi', 'pinverse', 'pixel_shuffle', 'pixel_unshuffle', 'platform', 'poisson', 'poisson_nll_loss', 'polar', 'polygamma', 'positive', 'pow', 'prelu', 'prepare_multiprocessing_environment', 'preserve_format', 'prev_error_mode', 'prod', 'profiler', 'promote_types', 'put', 'py_dll_path', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qint32', 'qint8', 'qr', 'qscheme', 'quantile', 'quantization', 'quantize_per_channel', 'quantize_per_tensor', 'quantize_per_tensor_dynamic', 'quantized_batch_norm', 'quantized_gru', 'quantized_gru_cell', 'quantized_lstm', 'quantized_lstm_cell', 'quantized_max_pool1d', 'quantized_max_pool2d', 'quantized_rnn_relu_cell', 'quantized_rnn_tanh_cell', 'quasirandom', 'quint2x4', 'quint4x2', 'quint8', 'rad2deg', 'rad2deg_', 'rand', 'rand_like', 'randint', 'randint_like', 'randn', 'randn_like', 'random', 'randperm', 'range', 'ravel', 'read_vitals', 'real', 'reciprocal', 'reciprocal_', 'relu', 'relu_', 'remainder', 'renorm', 'repeat_interleave', 'res', 'reshape', 'resize_as_', 'resize_as_sparse_', 'resolve_conj', 'resolve_neg', 'result_type', 'return_types', 'rnn_relu', 'rnn_relu_cell', 'rnn_tanh', 'rnn_tanh_cell', 'roll', 'rot90', 'round', 'round_', 'row_indices_copy', 'row_stack', 'rrelu', 'rrelu_', 'rsqrt', 'rsqrt_', 'rsub', 'saddmm', 'save', 'scalar_tensor', 'scatter', 'scatter_add', 'scatter_reduce', 'searchsorted', 'seed', 'segment_reduce', 'select', 'select_copy', 'select_scatter', 'selu', 'selu_', 'serialization', 'set_anomaly_enabled', 'set_autocast_cache_enabled', 'set_autocast_cpu_dtype', 'set_autocast_cpu_enabled', 'set_autocast_enabled', 'set_autocast_gpu_dtype', 'set_default_dtype', 'set_default_tensor_type', 'set_deterministic_debug_mode', 'set_float32_matmul_precision', 'set_flush_denormal', 'set_grad_enabled', 'set_num_interop_threads', 'set_num_threads', 'set_printoptions', 'set_rng_state', 'set_vital', 'set_warn_always', 'sgn', 'short', 'sigmoid', 'sigmoid_', 'sign', 'signbit', 'sin', 'sin_', 'sinc', 'sinc_', 'sinh', 'sinh_', 'slice_copy', 'slice_scatter', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse', 'sparse_bsc', 'sparse_bsc_tensor', 'sparse_bsr', 'sparse_bsr_tensor', 'sparse_compressed_tensor', 'sparse_coo', 'sparse_coo_tensor', 'sparse_csc', 'sparse_csc_tensor', 'sparse_csr', 'sparse_csr_tensor', 'special', 'split', 'split_copy', 'split_with_sizes', 'split_with_sizes_copy', 'spmm', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_copy', 'sspaddmm', 'stack', 'std', 'std_mean', 'stft', 'storage', 'strided', 'sub', 'subtract', 'sum', 'svd', 'svd_lowrank', 'swapaxes', 'swapdims', 'symeig', 'sys', 't', 't_copy', 'take', 'take_along_dim', 'tan', 'tan_', 'tanh', 'tanh_', 'tensor', 'tensor_split', 'tensordot', 'testing', 'textwrap', 'th_dll_path', 'threshold', 'threshold_', 'tile', 'to_dlpack', 'topk', 'torch', 'torch_version', 'trace', 'transpose', 'transpose_copy', 'trapezoid', 'trapz', 'triangular_solve', 'tril', 'tril_indices', 'triplet_margin_loss', 'triu', 'triu_indices', 'true_divide', 'trunc', 'trunc_', 'typename', 'types', 'uint8', 'unbind', 'unbind_copy', 'unfold_copy', 'unify_type_list', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_copy', 'use_deterministic_algorithms', 'utils', 'values_copy', 'vander', 'var', 'var_mean', 'vdot', 'version', 'view_as_complex', 'view_as_complex_copy', 'view_as_real', 'view_as_real_copy', 'view_copy', 'vitals_enabled', 'vsplit', 'vstack', 'wait', 'warnings', 'where', 'with_load_library_flags', 'xlogy', 'xlogy_', 'zero_', 'zeros', 'zeros_like']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa72d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function ones in module torch:\n",
      "\n",
      "ones(...)\n",
      "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "    by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword arguments:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.ones(2, 3)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "    \n",
      "        >>> torch.ones(5)\n",
      "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76bd26c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torch.autograd in torch:\n",
      "\n",
      "NAME\n",
      "    torch.autograd\n",
      "\n",
      "DESCRIPTION\n",
      "    ``torch.autograd`` provides classes and functions implementing automatic\n",
      "    differentiation of arbitrary scalar valued functions. It requires minimal\n",
      "    changes to the existing code - you only need to declare :class:`Tensor` s\n",
      "    for which gradients should be computed with the ``requires_grad=True`` keyword.\n",
      "    As of now, we only support autograd for floating point :class:`Tensor` types (\n",
      "    half, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _functions (package)\n",
      "    anomaly_mode\n",
      "    forward_ad\n",
      "    function\n",
      "    functional\n",
      "    grad_mode\n",
      "    gradcheck\n",
      "    graph\n",
      "    profiler\n",
      "    profiler_legacy\n",
      "    profiler_util\n",
      "    variable\n",
      "\n",
      "CLASSES\n",
      "    torch._C._FunctionBase(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function.FunctionCtx, torch.autograd.function._HookMixin)\n",
      "    torch._C._LegacyVariableBase(builtins.object)\n",
      "        torch.autograd.variable.Variable\n",
      "    torch.autograd.function.FunctionCtx(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function.FunctionCtx, torch.autograd.function._HookMixin)\n",
      "    torch.autograd.function._HookMixin(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function.FunctionCtx, torch.autograd.function._HookMixin)\n",
      "    \n",
      "    class Function(torch._C._FunctionBase, FunctionCtx, _HookMixin)\n",
      "     |  Function(*args, **kwargs)\n",
      "     |  \n",
      "     |  Base class to create custom `autograd.Function`\n",
      "     |  \n",
      "     |  To create a custom `autograd.Function`, subclass this class and implement\n",
      "     |  the :meth:`forward` and :meth:`backward` static methods. Then, to use your custom\n",
      "     |  op in the forward pass, call the class method ``apply``. Do not call\n",
      "     |  :meth:`forward` directly.\n",
      "     |  \n",
      "     |  To ensure correctness and best performance, make sure you are calling the\n",
      "     |  correct methods on ``ctx`` and validating your backward function using\n",
      "     |  :func:`torch.autograd.gradcheck`.\n",
      "     |  \n",
      "     |  See :ref:`extending-autograd` for more details on how to use this class.\n",
      "     |  \n",
      "     |  Examples::\n",
      "     |  \n",
      "     |      >>> class Exp(Function):\n",
      "     |      >>>     @staticmethod\n",
      "     |      >>>     def forward(ctx, i):\n",
      "     |      >>>         result = i.exp()\n",
      "     |      >>>         ctx.save_for_backward(result)\n",
      "     |      >>>         return result\n",
      "     |      >>>\n",
      "     |      >>>     @staticmethod\n",
      "     |      >>>     def backward(ctx, grad_output):\n",
      "     |      >>>         result, = ctx.saved_tensors\n",
      "     |      >>>         return grad_output * result\n",
      "     |      >>>\n",
      "     |      >>> # Use it by calling the apply method:\n",
      "     |      >>> output = Exp.apply(input)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Function\n",
      "     |      torch._C._FunctionBase\n",
      "     |      FunctionCtx\n",
      "     |      _HookMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwargs)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  backward(ctx: Any, *grad_outputs: Any) -> Any\n",
      "     |      Defines a formula for differentiating the operation with backward mode\n",
      "     |      automatic differentiation (alias to the vjp function).\n",
      "     |      \n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      \n",
      "     |      It must accept a context :attr:`ctx` as the first argument, followed by\n",
      "     |      as many outputs as the :func:`forward` returned (None will be passed in\n",
      "     |      for non tensor outputs of the forward function),\n",
      "     |      and it should return as many tensors, as there were inputs to\n",
      "     |      :func:`forward`. Each argument is the gradient w.r.t the given output,\n",
      "     |      and each returned value should be the gradient w.r.t. the\n",
      "     |      corresponding input. If an input is not a Tensor or is a Tensor not\n",
      "     |      requiring grads, you can just pass None as a gradient for that input.\n",
      "     |      \n",
      "     |      The context can be used to retrieve tensors saved during the forward\n",
      "     |      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n",
      "     |      of booleans representing whether each input needs gradient. E.g.,\n",
      "     |      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n",
      "     |      first input to :func:`forward` needs gradient computated w.r.t. the\n",
      "     |      output.\n",
      "     |  \n",
      "     |  forward(ctx: Any, *args: Any, **kwargs: Any) -> Any\n",
      "     |      Performs the operation.\n",
      "     |      \n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      \n",
      "     |      It must accept a context ctx as the first argument, followed by any\n",
      "     |      number of arguments (tensors or other types).\n",
      "     |      \n",
      "     |      The context can be used to store arbitrary data that can be then\n",
      "     |      retrieved during the backward pass. Tensors should not be stored\n",
      "     |      directly on `ctx` (though this is not currently enforced for\n",
      "     |      backward compatibility). Instead, tensors should be saved either with\n",
      "     |      :func:`ctx.save_for_backward` if they are intended to be used in\n",
      "     |      ``backward`` (equivalently, ``vjp``) or :func:`ctx.save_for_forward`\n",
      "     |      if they are intended to be used for in ``jvp``.\n",
      "     |  \n",
      "     |  jvp(ctx: Any, *grad_inputs: Any) -> Any\n",
      "     |      Defines a formula for differentiating the operation with forward mode\n",
      "     |      automatic differentiation.\n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      It must accept a context :attr:`ctx` as the first argument, followed by\n",
      "     |      as many inputs as the :func:`forward` got (None will be passed in\n",
      "     |      for non tensor inputs of the forward function),\n",
      "     |      and it should return as many tensors as there were outputs to\n",
      "     |      :func:`forward`. Each argument is the gradient w.r.t the given input,\n",
      "     |      and each returned value should be the gradient w.r.t. the\n",
      "     |      corresponding output. If an output is not a Tensor or the function is not\n",
      "     |      differentiable with respect to that output, you can just pass None as a\n",
      "     |      gradient for that input.\n",
      "     |      \n",
      "     |      You can use the :attr:`ctx` object to pass any value from the forward to this\n",
      "     |      functions.\n",
      "     |  \n",
      "     |  vjp = backward(ctx: Any, *grad_outputs: Any) -> Any\n",
      "     |      Defines a formula for differentiating the operation with backward mode\n",
      "     |      automatic differentiation (alias to the vjp function).\n",
      "     |      \n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      \n",
      "     |      It must accept a context :attr:`ctx` as the first argument, followed by\n",
      "     |      as many outputs as the :func:`forward` returned (None will be passed in\n",
      "     |      for non tensor outputs of the forward function),\n",
      "     |      and it should return as many tensors, as there were inputs to\n",
      "     |      :func:`forward`. Each argument is the gradient w.r.t the given output,\n",
      "     |      and each returned value should be the gradient w.r.t. the\n",
      "     |      corresponding input. If an input is not a Tensor or is a Tensor not\n",
      "     |      requiring grads, you can just pass None as a gradient for that input.\n",
      "     |      \n",
      "     |      The context can be used to retrieve tensors saved during the forward\n",
      "     |      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n",
      "     |      of booleans representing whether each input needs gradient. E.g.,\n",
      "     |      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n",
      "     |      first input to :func:`forward` needs gradient computated w.r.t. the\n",
      "     |      output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  is_traceable = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  name(...)\n",
      "     |  \n",
      "     |  register_hook(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  apply(...) from torch.autograd.function.FunctionMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  dirty_tensors\n",
      "     |  \n",
      "     |  materialize_grads\n",
      "     |  \n",
      "     |  metadata\n",
      "     |  \n",
      "     |  needs_input_grad\n",
      "     |  \n",
      "     |  next_functions\n",
      "     |  \n",
      "     |  non_differentiable\n",
      "     |  \n",
      "     |  requires_grad\n",
      "     |  \n",
      "     |  saved_for_forward\n",
      "     |  \n",
      "     |  saved_tensors\n",
      "     |  \n",
      "     |  saved_variables\n",
      "     |  \n",
      "     |  to_save\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FunctionCtx:\n",
      "     |  \n",
      "     |  mark_dirty(self, *args: torch.Tensor)\n",
      "     |      Marks given tensors as modified in an in-place operation.\n",
      "     |      \n",
      "     |      **This should be called at most once, only from inside the**\n",
      "     |      :func:`forward` **method, and all arguments should be inputs.**\n",
      "     |      \n",
      "     |      Every tensor that's been modified in-place in a call to :func:`forward`\n",
      "     |      should be given to this function, to ensure correctness of our checks.\n",
      "     |      It doesn't matter whether the function is called before or after\n",
      "     |      modification.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |          >>> class Inplace(Function):\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def forward(ctx, x):\n",
      "     |          >>>         x_npy = x.numpy() # x_npy shares storage with x\n",
      "     |          >>>         x_npy += 1\n",
      "     |          >>>         ctx.mark_dirty(x)\n",
      "     |          >>>         return x\n",
      "     |          >>>\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     @once_differentiable\n",
      "     |          >>>     def backward(ctx, grad_output):\n",
      "     |          >>>         return grad_output\n",
      "     |          >>>\n",
      "     |          >>> a = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n",
      "     |          >>> b = a * a\n",
      "     |          >>> Inplace.apply(a)  # This would lead to wrong gradients!\n",
      "     |          >>>                   # but the engine would not know unless we mark_dirty\n",
      "     |          >>> b.backward() # RuntimeError: one of the variables needed for gradient\n",
      "     |          >>>              # computation has been modified by an inplace operation\n",
      "     |  \n",
      "     |  mark_non_differentiable(self, *args: torch.Tensor)\n",
      "     |      Marks outputs as non-differentiable.\n",
      "     |      \n",
      "     |      **This should be called at most once, only from inside the**\n",
      "     |      :func:`forward` **method, and all arguments should be tensor outputs.**\n",
      "     |      \n",
      "     |      This will mark outputs as not requiring gradients, increasing the\n",
      "     |      efficiency of backward computation. You still need to accept a gradient\n",
      "     |      for each output in :meth:`~Function.backward`, but it's always going to\n",
      "     |      be a zero tensor with the same shape as the shape of a corresponding\n",
      "     |      output.\n",
      "     |      \n",
      "     |      This is used e.g. for indices returned from a sort. See example::\n",
      "     |          >>> class Func(Function):\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def forward(ctx, x):\n",
      "     |          >>>         sorted, idx = x.sort()\n",
      "     |          >>>         ctx.mark_non_differentiable(idx)\n",
      "     |          >>>         ctx.save_for_backward(x, idx)\n",
      "     |          >>>         return sorted, idx\n",
      "     |          >>>\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     @once_differentiable\n",
      "     |          >>>     def backward(ctx, g1, g2):  # still need to accept g2\n",
      "     |          >>>         x, idx = ctx.saved_tensors\n",
      "     |          >>>         grad_input = torch.zeros_like(x)\n",
      "     |          >>>         grad_input.index_add_(0, idx, g1)\n",
      "     |          >>>         return grad_input\n",
      "     |  \n",
      "     |  mark_shared_storage(self, *pairs)\n",
      "     |  \n",
      "     |  save_for_backward(self, *tensors: torch.Tensor)\n",
      "     |      Saves given tensors for a future call to :func:`~Function.backward`.\n",
      "     |      \n",
      "     |      ``save_for_backward`` should be called at most once, only from inside the\n",
      "     |      :func:`forward` method, and only with tensors.\n",
      "     |      \n",
      "     |      All tensors intended to be used in the backward pass should be saved\n",
      "     |      with ``save_for_backward`` (as opposed to directly on ``ctx``) to prevent\n",
      "     |      incorrect gradients and memory leaks, and enable the application of saved\n",
      "     |      tensor hooks. See :class:`torch.autograd.graph.saved_tensors_hooks`.\n",
      "     |      \n",
      "     |      In :func:`backward`, saved tensors can be accessed through the :attr:`saved_tensors`\n",
      "     |      attribute. Before returning them to the user, a check is made to ensure\n",
      "     |      they weren't used in any in-place operation that modified their content.\n",
      "     |      \n",
      "     |      Arguments can also be ``None``. This is a no-op.\n",
      "     |      \n",
      "     |      See :ref:`extending-autograd` for more details on how to use this method.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |          >>> class Func(Function):\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n",
      "     |          >>>         w = x * y * z\n",
      "     |          >>>         out = x * y + y * z + w\n",
      "     |          >>>         ctx.save_for_backward(x, y, w, out)\n",
      "     |          >>>         ctx.z = z  # z is not a tensor\n",
      "     |          >>>         return out\n",
      "     |          >>>\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def backward(ctx, grad_out):\n",
      "     |          >>>         x, y, w, out = ctx.saved_tensors\n",
      "     |          >>>         z = ctx.z\n",
      "     |          >>>         gx = grad_out * (y + y * z)\n",
      "     |          >>>         gy = grad_out * (x + z + x * z)\n",
      "     |          >>>         gz = None\n",
      "     |          >>>         return gx, gy, gz\n",
      "     |          >>>\n",
      "     |          >>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n",
      "     |          >>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n",
      "     |          >>> c = 4\n",
      "     |          >>> d = Func.apply(a, b, c)\n",
      "     |  \n",
      "     |  save_for_forward(self, *tensors: torch.Tensor)\n",
      "     |      Saves given tensors for a future call to :func:`~Function.jvp`.\n",
      "     |      \n",
      "     |      ``save_for_forward`` should be only called once, from inside the :func:`forward`\n",
      "     |      method, and only be called with tensors.\n",
      "     |      \n",
      "     |      In :func:`jvp`, saved objects can be accessed through the :attr:`saved_tensors`\n",
      "     |      attribute.\n",
      "     |      \n",
      "     |      Arguments can also be ``None``. This is a no-op.\n",
      "     |      \n",
      "     |      See :ref:`extending-autograd` for more details on how to use this method.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |          >>> class Func(torch.autograd.Function):\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n",
      "     |          >>>         ctx.save_for_backward(x, y)\n",
      "     |          >>>         ctx.save_for_forward(x, y)\n",
      "     |          >>>         ctx.z = z\n",
      "     |          >>>         return x * y * z\n",
      "     |          >>>\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def jvp(ctx, x_t, y_t, _):\n",
      "     |          >>>         x, y = ctx.saved_tensors\n",
      "     |          >>>         z = ctx.z\n",
      "     |          >>>         return z * (y * x_t + x * y_t)\n",
      "     |          >>>\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def vjp(ctx, grad_out):\n",
      "     |          >>>         x, y = ctx.saved_tensors\n",
      "     |          >>>         z = ctx.z\n",
      "     |          >>>         return z * grad_out * y, z * grad_out * x, None\n",
      "     |          >>>\n",
      "     |          >>>     a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n",
      "     |          >>>     t = torch.tensor(1., dtype=torch.double)\n",
      "     |          >>>     b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n",
      "     |          >>>     c = 4\n",
      "     |          >>>\n",
      "     |          >>>     with fwAD.dual_level():\n",
      "     |          >>>         a_dual = fwAD.make_dual(a, t)\n",
      "     |          >>>         d = Func.apply(a_dual, b, c)\n",
      "     |  \n",
      "     |  set_materialize_grads(self, value: bool)\n",
      "     |      Sets whether to materialize output grad tensors. Default is ``True``.\n",
      "     |      \n",
      "     |      **This should be called only from inside the** :func:`forward` **method**\n",
      "     |      \n",
      "     |      If ``True``, undefined output grad tensors will be expanded to tensors full\n",
      "     |      of zeros prior to calling the :func:`backward` method.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |          >>> class SimpleFunc(Function):\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def forward(ctx, x):\n",
      "     |          >>>         return x.clone(), x.clone()\n",
      "     |          >>>\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     @once_differentiable\n",
      "     |          >>>     def backward(ctx, g1, g2):\n",
      "     |          >>>         return g1 + g2  # No check for None necessary\n",
      "     |          >>>\n",
      "     |          >>> # We modify SimpleFunc to handle non-materialized grad outputs\n",
      "     |          >>> class Func(Function):\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     def forward(ctx, x):\n",
      "     |          >>>         ctx.set_materialize_grads(False)\n",
      "     |          >>>         ctx.save_for_backward(x)\n",
      "     |          >>>         return x.clone(), x.clone()\n",
      "     |          >>>\n",
      "     |          >>>     @staticmethod\n",
      "     |          >>>     @once_differentiable\n",
      "     |          >>>     def backward(ctx, g1, g2):\n",
      "     |          >>>         x, = ctx.saved_tensors\n",
      "     |          >>>         grad_input = torch.zeros_like(x)\n",
      "     |          >>>         if g1 is not None:  # We must check for None now\n",
      "     |          >>>             grad_input += g1\n",
      "     |          >>>         if g2 is not None:\n",
      "     |          >>>             grad_input += g2\n",
      "     |          >>>         return grad_input\n",
      "     |          >>>\n",
      "     |          >>> a = torch.tensor(1., requires_grad=True)\n",
      "     |          >>> b, _ = Func.apply(a)  # induces g2 to be undefined\n",
      "    \n",
      "    class Variable(torch._C._LegacyVariableBase)\n",
      "     |  # mypy doesn't understand torch._six.with_metaclass\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Variable\n",
      "     |      torch._C._LegacyVariableBase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch._C._LegacyVariableBase:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "FUNCTIONS\n",
      "    backward(tensors: Union[torch.Tensor, Sequence[torch.Tensor]], grad_tensors: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, inputs: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None) -> None\n",
      "        Computes the sum of gradients of given tensors with respect to graph\n",
      "        leaves.\n",
      "        \n",
      "        The graph is differentiated using the chain rule. If any of ``tensors``\n",
      "        are non-scalar (i.e. their data has more than one element) and require\n",
      "        gradient, then the Jacobian-vector product would be computed, in this\n",
      "        case the function additionally requires specifying ``grad_tensors``.\n",
      "        It should be a sequence of matching length, that contains the \"vector\"\n",
      "        in the Jacobian-vector product, usually the gradient of the differentiated\n",
      "        function w.r.t. corresponding tensors (``None`` is an acceptable value for\n",
      "        all tensors that don't need gradient tensors).\n",
      "        \n",
      "        This function accumulates gradients in the leaves - you might need to zero\n",
      "        ``.grad`` attributes or set them to ``None`` before calling it.\n",
      "        See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      "        for details on the memory layout of accumulated gradients.\n",
      "        \n",
      "        .. note::\n",
      "            Using this method with ``create_graph=True`` will create a reference cycle\n",
      "            between the parameter and its gradient which can cause a memory leak.\n",
      "            We recommend using ``autograd.grad`` when creating the graph to avoid this.\n",
      "            If you have to use this function, make sure to reset the ``.grad`` fields of your\n",
      "            parameters to ``None`` after use to break the cycle and avoid the leak.\n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``\n",
      "            in a user-specified CUDA stream context, see\n",
      "            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            When ``inputs`` are provided and a given input is not a leaf,\n",
      "            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).\n",
      "            It is an implementation detail on which the user should not rely.\n",
      "            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
      "        \n",
      "        Args:\n",
      "            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be\n",
      "                computed.\n",
      "            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The \"vector\" in\n",
      "                the Jacobian-vector product, usually gradients w.r.t. each element of\n",
      "                corresponding tensors. None values can be specified for scalar Tensors or\n",
      "                ones that don't require grad. If a None value would be acceptable for all\n",
      "                grad_tensors, then this argument is optional.\n",
      "            retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n",
      "                will be freed. Note that in nearly all cases setting this option to ``True``\n",
      "                is not needed and often can be worked around in a much more efficient\n",
      "                way. Defaults to the value of ``create_graph``.\n",
      "            create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "                be constructed, allowing to compute higher order derivative products.\n",
      "                Defaults to ``False``.\n",
      "            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient\n",
      "                be will accumulated into ``.grad``. All other Tensors will be ignored. If\n",
      "                not provided, the gradient is accumulated into all the leaf Tensors that\n",
      "                were used to compute the attr::tensors.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Variable', 'Function', 'backward', 'grad_mode']\n",
      "\n",
      "FILE\n",
      "    c:\\users\\david\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e0065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
